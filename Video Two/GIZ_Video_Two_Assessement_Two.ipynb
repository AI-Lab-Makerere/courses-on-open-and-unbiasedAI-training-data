{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assessment-two-notebook-video2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LT2VhBKUap7"
      },
      "source": [
        "#**Example Machine Learning application of learning a crop type classifiction model using training data from Radiant ML Hub**\n",
        "\n",
        "This Python notebook takes you through an application of learning a classification model for classifying crop types against field IDs in a satellite image.\n",
        "\n",
        "The notebook is based on a starter notebook provided for the 2020 ICLR workshop challenge at Zindi on using earth observation data for classifying crop types during the growing season: https://zindi.africa/competitions/iclr-workshop-challenge-2-radiant-earth-computer-vision-for-crop-recognition\n",
        "\n",
        "---\n",
        "This is a high level overview of the notebook:\n",
        "\n",
        "- Download data (tif files and label and field information) from Radiant ML Hub\n",
        "- Do data preprocessing by sampling the satellite images to get the data into an easy tabular form \n",
        "- Learn a classification model on this data, and experiment with grouping by field or doing pixel-wise classifications.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7XY_Eh7U9HW"
      },
      "source": [
        "## **Download Data from Radiant ML Hub**\n",
        "\n",
        "At the end of this section you should have downloaded all images and labels of the crop type dataset that constitutes satellite imagery over western Kenya labeled with field IDs and respective crop types.\n",
        "\n",
        "The code in this section is essentially a clone from a starter notebook provided by Radiant Earth for downloading satellite images labeled with field IDs and crop types: https://github.com/radiantearth/mlhub-tutorials/tree/master/notebooks/2020%20CV4A%20Crop%20Type%20Challenge\n",
        "\n",
        "In this section we will:\n",
        "\n",
        "- install the new radiant mlhub client.\n",
        "- import libraries that we use to access and get information from the Radiant ML Hub API.\n",
        "- specify authentication details and API properties that we shall use whenever we access the API.\n",
        "- specify the download location where the dataset will be downloaded.\n",
        "- define functions to use for donwloading labels and images.\n",
        "- specify collection IDs and download the respective labels and images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAb8QY2KVqB1"
      },
      "source": [
        "###**Install Radiant MLHub client**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1es5d8IUMrX"
      },
      "source": [
        "!pip install radiant_mlhub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4F1_Rr_Vx3U"
      },
      "source": [
        "###**Import necessary packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JANnH2cSV2YP"
      },
      "source": [
        "#import urllib.parse\n",
        "#import re\n",
        "#from pathlib import Path\n",
        "#import itertools as it\n",
        "#from functools import partial\n",
        "#from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "#from tqdm.notebook import tqdm\n",
        "#from radiant_mlhub import client, get_session\n",
        "\n",
        "from radiant_mlhub import Dataset, client\n",
        "import tarfile\n",
        "from pathlib import Path"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbo8cNHQV_1e"
      },
      "source": [
        "###**Authentication**\n",
        "\n",
        "You need an API_KEY to be able to download some datasets. \n",
        "\n",
        "You can get API_KEY from your account on Radiant MLHUB.\n",
        "\n",
        "If you do not have an account, go to [https://www.mlhub.earth/](https://www.mlhub.earth/) and sign up for API access. \n",
        "\n",
        "If you have an account, you can login and get an API_KEY through the API_KEY menu at [https://dashboard.mlhub.earth/](https://dashboard.mlhub.earth/). \n",
        "\n",
        "You can then paste your API_KEY in this cell at 'PASTE YOUR API_KEY HERE'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1_UeQoEWNww"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ['MLHUB_API_KEY'] = 'PASTE YOUR API_KEY HERE'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR6rV9FVYlmS"
      },
      "source": [
        "### **Retrieving the data**\n",
        "\n",
        "Datasets are stored as collections on Radiant MLHub catalog. A collection represents the top-most data level. Typically this means the data comes from the same source for the same geography. It might include different years or sub-geographies.\n",
        "\n",
        "The two collections we shall use in the machine learning application are:\n",
        "\n",
        "- ref_african_crops_kenya_02_source: includes the multi-temporal bands of Sentinel-2\n",
        "- ref_african_crops_kenya_02_labels: includes the labels and field IDs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48uRzZCIYfZd"
      },
      "source": [
        "dataset = Dataset.fetch('ref_african_crops_kenya_02')\n",
        "\n",
        "print(f'ID: {dataset.id}')\n",
        "print(f'Title: {dataset.title}')\n",
        "print('Collections:')\n",
        "for collection in dataset.collections:\n",
        "    print(f'* {collection.id}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0z6CqYHjIYf"
      },
      "source": [
        "# output path where you want to download the data\n",
        "output_path = Path(\"./data/\").resolve()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QWyfF6xjNah"
      },
      "source": [
        "archive_paths = dataset.download(output_dir=output_path)\n",
        "for archive_path in archive_paths:\n",
        "    print(f'Extracting {archive_path}...')\n",
        "    with tarfile.open(archive_path) as tfile:\n",
        "        tfile.extractall(path=output_path)\n",
        "\n",
        "print('Done\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATdjZMpCUUqm"
      },
      "source": [
        "##**Viewing the Images**\n",
        "\n",
        "In this section, we will explore the downloaded data set by viewing a single image corresponding one spectral band, then we will view an RGB image from a combination of images corresponding to the RGB colors. \n",
        "\n",
        "Again, this section of the notebook contains code from the starter notebooks on loading and visualizing a sample of .tif images from Radiant ML Hub. \n",
        "\n",
        "In the code, we use the tifffile package to read .tif images. We use the matplotlib library to view the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kohvDQDhywlf"
      },
      "source": [
        "# Required libraries\n",
        "from pathlib import Path\n",
        "from io import BytesIO\n",
        "from glob import glob\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import tifffile as tiff\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage import exposure\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU09bsvuy08k"
      },
      "source": [
        "# The path to the source collection that was extracted from the download archive\n",
        "collection_path = Path('./data/ref_african_crops_kenya_02_source')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v41Qpkuoy4Yf"
      },
      "source": [
        "def load_file(img_path):\n",
        "    \"\"\"Takes a path to the download archive and the path within the archive to the image and returns a numpy array.\"\"\"\n",
        "    \n",
        "    return tiff.imread(str(img_path))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNQmhQ6Hy-Nu"
      },
      "source": [
        "# Get a list of dates that an observation from Sentinel-2 is provided for from the currently \n",
        "#  downloaded imagery\n",
        "\n",
        "tile_dates = dict()\n",
        "\n",
        "def filter_tifs(name):\n",
        "    return name.endswith('.tif')\n",
        "\n",
        "for f in glob(str(collection_path / '**/*.tif')):\n",
        "    tif_path = Path(f)\n",
        "    _, tile_id, tile_date = tif_path.parent.name.rsplit('_', 2)\n",
        "\n",
        "    if tile_id not in tile_dates:\n",
        "        tile_dates[tile_id] = set()\n",
        "\n",
        "    tile_dates[tile_id].add(tile_date)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvmB83BIzBjE"
      },
      "source": [
        "for tile in sorted(tile_dates.keys()):\n",
        "    print(f'Tile ID: {tile}')\n",
        "    dates = sorted(list(tile_dates[tile]))\n",
        "    print(f'Dates: {\", \".join(dates)}')\n",
        "    print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW0vbp12zHlO"
      },
      "source": [
        "# Available bands\n",
        "bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12', 'CLD']"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbxHqzT5zIaB"
      },
      "source": [
        "# Sample file to load:\n",
        "tile = '02'\n",
        "date_ = '20190825'\n",
        "band = 'B03'\n",
        "\n",
        "archive_path = \"./data/ref_african_crops_kenya_02_source.tar.gz\"\n",
        "img_path = collection_path / f\"ref_african_crops_kenya_02_tile_{tile}_{date_}/{band}.tif\"\n",
        "band_data = load_file(img_path)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74GQww2QzWK4"
      },
      "source": [
        "fig = plt.figure(figsize=(7, 7))\n",
        "plt.imshow(band_data, vmin=0, vmax=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBmDUk-w02il"
      },
      "source": [
        "##**Data preprocessing for machine learning**\n",
        "\n",
        "Now that you have downloaded and explored the dataset by viewing a sample of images, let's now transform the downloaded data for training and evaluating classificatnon models. \n",
        "\n",
        "First of all, we need to get all the dates constituting the observations in the dataset and the bands in each observation into lists that we shall use later to specify images or go through all images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1awcupL1DBh"
      },
      "source": [
        "import datetime\n",
        "# List of dates that an observation from Sentinel-2 is provided in the training dataset\n",
        "\n",
        "dates = [datetime.datetime(2019, 6, 6, 8, 10, 7),\n",
        "         datetime.datetime(2019, 7, 1, 8, 10, 4),\n",
        "         datetime.datetime(2019, 7, 6, 8, 10, 8),\n",
        "         datetime.datetime(2019, 7, 11, 8, 10, 4),\n",
        "         datetime.datetime(2019, 7, 21, 8, 10, 4),\n",
        "         datetime.datetime(2019, 8, 5, 8, 10, 7),\n",
        "         datetime.datetime(2019, 8, 15, 8, 10, 6),\n",
        "         datetime.datetime(2019, 8, 25, 8, 10, 4),\n",
        "         datetime.datetime(2019, 9, 9, 8, 9, 58),\n",
        "         datetime.datetime(2019, 9, 19, 8, 9, 59),\n",
        "         datetime.datetime(2019, 9, 24, 8, 9, 59),\n",
        "         datetime.datetime(2019, 10, 4, 8, 10),\n",
        "         datetime.datetime(2019, 11, 3, 8, 10)]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9zOfvRX1Hnj"
      },
      "source": [
        "# List of bands in each observation\n",
        "\n",
        "bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12', 'CLD']"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoGEy_3e1fpq"
      },
      "source": [
        "###**Getting data for each pixel**\n",
        "\n",
        "This is where we start massaging the data into a format we can use.\n",
        "\n",
        "We start by reading in the labels, finding the pixel locations of all the fields (most are only a few pixels in size) and storing the pixel locations, the field ID and the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7w_n_4p1I2S"
      },
      "source": [
        "# Not super efficient but  ¯\\_(ツ)_/¯\n",
        "import pandas as pd\n",
        "\n",
        "row_locs = []\n",
        "col_locs = []\n",
        "field_ids = []\n",
        "labels = []\n",
        "tiles = []\n",
        "for tile in range(4):\n",
        "  fids = f'./data/ref_african_crops_kenya_02_labels/ref_african_crops_kenya_02_tile_0{tile}_label/field_ids.tif'\n",
        "  labs = f'./data/ref_african_crops_kenya_02_labels/ref_african_crops_kenya_02_tile_0{tile}_label/labels.tif'\n",
        "  fid_arr = load_file(fids)\n",
        "  lab_arr = load_file(labs)\n",
        "  for row in range(len(fid_arr)):\n",
        "    for col in range(len(fid_arr[0])):\n",
        "      if fid_arr[row][col] != 0:\n",
        "        row_locs.append(row)\n",
        "        col_locs.append(col)\n",
        "        field_ids.append(fid_arr[row][col])\n",
        "        labels.append(lab_arr[row][col])\n",
        "        tiles.append(tile)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'fid':field_ids,\n",
        "    'label':labels,\n",
        "    'row_loc': row_locs,\n",
        "    'col_loc':col_locs,\n",
        "    'tile':tiles\n",
        "})\n",
        "\n",
        "print(df.shape)\n",
        "print(df.groupby('fid').count().shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3m_ALaY11qX"
      },
      "source": [
        "---\n",
        "\n",
        "Then we loop through all the images, sampling the different image bands to get the values for each pixel in each field.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ66NGmS13LC"
      },
      "source": [
        "# Sample the bands at different dates as columns in our new dataframe\n",
        "col_names = []\n",
        "col_values = []\n",
        "\n",
        "for tile in range(4): # 1) For each tile\n",
        "  print('Tile: ', tile)\n",
        "  for d in dates: # 2) For each date\n",
        "    print(str(d))\n",
        "    d = ''.join(str(d.date()).split('-')) # Nice date string\n",
        "    t = '0'+str(tile)\n",
        "    for b in bands: # 3) For each band\n",
        "      col_name = d+'_'+b\n",
        "      \n",
        "      if tile == 0:\n",
        "        # If the column doesn't exist, create it and populate with 0s\n",
        "        df[col_name] = 0\n",
        "\n",
        "      # Load im\n",
        "      im = load_file(f\"./data/ref_african_crops_kenya_02_source/ref_african_crops_kenya_02_tile_{t}_{d}/{b}.tif\")\n",
        "      \n",
        "      # Going four levels deep. Each second on the outside is four weeks in this loop\n",
        "      # If we die here, there's no waking up.....\n",
        "      vals = []\n",
        "      for row, col in df.loc[df.tile == tile][['row_loc', 'col_loc']].values: # 4) For each location of a pixel in a field\n",
        "        vals.append(im[row][col])\n",
        "      df.loc[df.tile == tile, col_name] = vals\n",
        "df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrv39WJq2BHK"
      },
      "source": [
        "df.to_csv('sampled_data.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVBPFIEK2E6y"
      },
      "source": [
        "---\n",
        "\n",
        "We can copy the prepared data (sampled_data.csv) to the data folder in Google Drive\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGHwqXUG2GBe"
      },
      "source": [
        "!cp 'sampled_data.csv' '/content/drive/My Drive/Colab Notebooks/data/sampled_data.csv' # You can mount drive and save like this, or just download sampled_data.csv and use that later."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilTBKS7R2P2r"
      },
      "source": [
        "# **Using the prepared data for machine learning crop type classification models**\n",
        "\n",
        "---\n",
        "\n",
        "Now that we have our data in a nice tabular data format, we can try some simple machine learning methods using the sklearn package.\n",
        "\n",
        "We can restart the runtime here and load in the data we saved earlier.\n",
        "\n",
        "---\n",
        "\n",
        "### **Import classifiers from sklearn**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRE98ElG2VbA"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier # Random Forest classifier, AdaBoost classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier # K nearest neighbor classifier\n",
        "from sklearn.tree import DecisionTreeClassifier # Decision tree classifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier # Gaussian process classifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.metrics import log_loss # evaluation metric"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9UoAIZF2cjI"
      },
      "source": [
        "### **Load data set**\n",
        "\n",
        "We load the data set and split it into a training set (for training a classifier) and a test set (for evaluating the classifier).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjHV3yY6DW5_"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jL6Y_cW2fpy"
      },
      "source": [
        "# Load the data\n",
        "df = pd.read_csv('./sampled_data.csv')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzKvFpov2j1G",
        "outputId": "7f924c1b-a211-4f59-aaba-133c831006a6"
      },
      "source": [
        "# Split into train and test sets\n",
        "train = df.loc[df.label != 0]\n",
        "test =  df.loc[df.label == 0]\n",
        "train.shape, test.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((47791, 174), (19766, 174))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rvvZ1dk2nqb"
      },
      "source": [
        "# Split train into train and validation (val) set, so that we can score our models locally.\n",
        "\n",
        "# We split on field ID since we want to predict for unseen FIELDS\n",
        "val_field_ids = train.groupby('fid').mean().reset_index()['fid'].sample(frac=0.3).values\n",
        "tr = train.loc[~train.fid.isin(val_field_ids)].copy()\n",
        "val = train.loc[train.fid.isin(val_field_ids)].copy()\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kjlWMwz2uaK"
      },
      "source": [
        "### **Classification model training**\n",
        "\n",
        "We can train a classifier to predict on pixels and then combine, or we can train a classfier by first grouping into fields. Let's do both and compare!\n",
        "\n",
        "**A. Training and predicting on pixels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAqPFv3p2zpV"
      },
      "source": [
        "# Split into X and Y for modelling\n",
        "X_train, y_train = tr[tr.columns[5:]], tr['label']\n",
        "X_val, y_val = val[val.columns[5:]], val['label']"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlXDh-xy23OD"
      },
      "source": [
        "# Train a classification model (takes a few minutes since we have plenty of rows)\n",
        "model = RandomForestClassifier(n_estimators=500)\n",
        "model.fit(X_train.fillna(0), y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjorrSdh26OR"
      },
      "source": [
        "# Predict on pixels\n",
        "\n",
        "# We use the predict_proba() function to get predicted probabilities\n",
        "preds = model.predict_proba(X_val.fillna(0))\n",
        "\n",
        "# Add to val dataframe as columns\n",
        "for i in range(7):\n",
        "  val[str(i+1)] = preds[:,i]\n",
        "\n",
        "# Get 'true' vals as columns in val\n",
        "for c in range(1, 8):\n",
        "  val['true'+str(c)] = (val['label'] == c).astype(int)\n",
        "\n",
        "pred_cols = [str(i) for i in range(1, 8)]\n",
        "true_cols = ['true'+str(i) for i in range(1, 8)]\n",
        "\n",
        "# Calculate the evaluation score - using the log-loss metric\n",
        "#print('Pixel score:', log_loss(val[true_cols], val[pred_cols]))\n",
        "print('Field score:', log_loss(val.groupby('fid').mean()[true_cols], val.groupby('fid').mean()[pred_cols])) # This is what we'll use to compare\n",
        "\n",
        "# Inspect or compare visually\n",
        "val[['label']+true_cols+pred_cols].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i4dTtGM2-4s"
      },
      "source": [
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "**B. Training and predicting on fields**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Crong7Y22_2L"
      },
      "source": [
        "# Group training data into fields\n",
        "\n",
        "train_grouped = tr.groupby('fid').mean().reset_index()\n",
        "val_grouped = val.groupby('fid').mean().reset_index()\n",
        "X_train, y_train = train_grouped[train_grouped.columns[5:]], train_grouped['label']\n",
        "X_val, y_val = val_grouped[train_grouped.columns[5:]], val_grouped['label']"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djrTrGJ43FaU"
      },
      "source": [
        "# Train a classification model\n",
        "model = RandomForestClassifier(n_estimators=500)\n",
        "model.fit(X_train.fillna(0), y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Hp6cZA73GWh"
      },
      "source": [
        "# Get predicted probabilities\n",
        "preds = model.predict_proba(X_val.fillna(0))\n",
        "\n",
        "# Add to val_grouped dataframe as columns\n",
        "for i in range(7):\n",
        "  val_grouped[str(i+1)] = preds[:,i]\n",
        "\n",
        "# Get 'true' vals as columns in val\n",
        "for c in range(1, 8):\n",
        "  val_grouped['true'+str(c)] = (val_grouped['label'] == c).astype(int)\n",
        "\n",
        "pred_cols = [str(i) for i in range(1, 8)]\n",
        "true_cols = ['true'+str(i) for i in range(1, 8)]\n",
        "val_grouped[['label']+true_cols+pred_cols].head()\n",
        "\n",
        "# Already grouped, but just to double check:\n",
        "print('Field score:', log_loss(val_grouped.groupby('fid').mean()[true_cols], val_grouped.groupby('fid').mean()[pred_cols]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2n3jojWFIGz"
      },
      "source": [
        "## **PRACTICAL EXERCISE**\n",
        "\n",
        "### **If you were able to complete train and predict in A and B, answer the following questions**\n",
        "\n",
        "#### **Qn 1.** Which of the two approaches is much faster?\n",
        "\n",
        "#### **Qn 2.** Which of the two approaches is more accurate?\n",
        "\n",
        "#### **Qn 3 - Hands on.** Instead of the Random Forest classifier, apply other classifiers from the sklearn package and compare their performance using the log loss metric."
      ]
    }
  ]
}